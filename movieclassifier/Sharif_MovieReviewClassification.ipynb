{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyprind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No valid output stream.\n"
     ]
    }
   ],
   "source": [
    "basepath = '/Users/jamilsharif/Desktop/aclImdb'\n",
    "labels = {'pos': 1, 'neg': 0}\n",
    "pbar = pyprind.ProgBar(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a DataFrame to store the data\n",
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []  # Initialize an empty list to store DataFrames\n",
    "# Loop over the 'test' and 'train' directories\n",
    "for s in ('test', 'train'):\n",
    "    # Loop over the 'pos' and 'neg' subdirectories\n",
    "    for l in ('pos', 'neg'):\n",
    "        # Construct the path to the directory containing the files\n",
    "        path = os.path.join(basepath, s, l)\n",
    "        try:\n",
    "            # Get the list of files in the directory\n",
    "            files = sorted(os.listdir(path))\n",
    "        except FileNotFoundError:\n",
    "            # Handle the case where the directory is not found\n",
    "            print(f\"Directory '{path}' not found.\")\n",
    "            continue\n",
    "        # Initialize a list to store data from current directory\n",
    "        data = []\n",
    "        # Loop over the files in the directory\n",
    "        for file in files:\n",
    "            # Read the contents of the file\n",
    "            with open(os.path.join(path, file), 'r', encoding='utf-8') as infile:\n",
    "                txt = infile.read()\n",
    "            # Append the text and label to the list\n",
    "            data.append([txt, labels[l]])\n",
    "            # Update the progress bar\n",
    "            pbar.update()\n",
    "        # Convert the list to a DataFrame and append to the list of DataFrames\n",
    "        dfs.append(pd.DataFrame(data, columns=['review', 'sentiment']))\n",
    "\n",
    "# Concatenate all DataFrames in the list\n",
    "df = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the DataFrame\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "df.to_csv('/Users/jamilsharif/SentimentClassification/movie_data.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  sentiment\n",
      "0  Calling this a romantic comedy is accurate but...          1\n",
      "1  I'm not aware of \"Largo Winch\" as a comic book...          1\n",
      "2  The stranger Jack (Matthew Lillard) arrives in...          0\n",
      "3  This film fails on many many levels. The scrip...          0\n",
      "4  Gregory Peck gives a brilliant performance in ...          1\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/jamilsharif/SentimentClassification/movie_data.csv', encoding='utf-8')\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47864, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count = CountVectorizer()\n",
    "docs = np.array(['The sun is shining',\n",
    "                 'The weather is sweet',\n",
    "                 'The sun is shining, the weather is sweet, and one and one is two'])\n",
    "bag = count.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 6, 'sun': 4, 'is': 1, 'shining': 3, 'weather': 8, 'sweet': 5, 'and': 0, 'one': 2, 'two': 7}\n"
     ]
    }
   ],
   "source": [
    "print(count.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.43 0.   0.56 0.56 0.   0.43 0.   0.  ]\n",
      " [0.   0.43 0.   0.   0.   0.56 0.43 0.   0.56]\n",
      " [0.5  0.45 0.5  0.19 0.19 0.19 0.3  0.25 0.19]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer(use_idf=True, norm='l2', smooth_idf=True)\n",
    "np.set_printoptions(precision=2)\n",
    "print(tfidf.fit_transform(count.fit_transform(docs)).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'genre and borrows a plot element, but that is all.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, 'review'][-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessor function to clean the text data\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) +\n",
    "            ' '.join(emoticons).replace('-', ''))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genre and borrows a plot element but that is all \n",
      "this is a test :) :( :)\n"
     ]
    }
   ],
   "source": [
    "# Test the preprocessor function\n",
    "print(preprocessor(df.loc[0, 'review'][-50:]))\n",
    "print(preprocessor(\"</a>This :) is :( a test :-)!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessor function to all reviews in the DataFrame\n",
    "df['review'] = df['review'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tokenizer function to split documents into individual words\n",
    "def tokenizer(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['runners', 'like', 'running', 'and', 'thus', 'they', 'run']\n"
     ]
    }
   ],
   "source": [
    "# Test the tokenizer function\n",
    "print(tokenizer('runners like running and thus they run'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tokenizer function with Porter stemming\n",
    "porter = PorterStemmer()\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['runner', 'like', 'run', 'run', 'lot']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download English stop-words\n",
    "# nltk.download('stopwords') # comment this out to download and then comment back\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load English stop-words\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# Example text\n",
    "text = \"a runner likes running and runs a lot\"\n",
    "\n",
    "# Tokenize text and remove stop-words\n",
    "filtered_words = [w for w in tokenizer_porter(text) if w not in stop]\n",
    "\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.loc[:5000, 'review'].values\n",
    "y_train = df.loc[:5000, 'sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=None; total time=   3.3s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=None; total time=   3.4s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=None; total time=   3.4s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=None; total time=   3.4s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=None; total time=   2.8s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 2), vect__stop_words=None, vect__tokenizer=None; total time=  11.6s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 2), vect__stop_words=None, vect__tokenizer=None; total time=  11.7s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 2), vect__stop_words=None, vect__tokenizer=None; total time=  11.8s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 2), vect__stop_words=None, vect__tokenizer=None; total time=  11.6s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=None; total time=   2.6s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=None; total time=   2.6s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=None; total time=   2.7s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=None; total time=   2.9s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=None; total time=   2.7s\n",
      "[CV] END clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 2), vect__stop_words=None, vect__tokenizer=None; total time=  12.2s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 2), vect__stop_words=None, vect__tokenizer=None; total time=  11.7s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 2), vect__stop_words=None, vect__tokenizer=None; total time=  11.8s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 2), vect__stop_words=None, vect__tokenizer=None; total time=  11.9s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=None; total time=   3.2s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=None; total time=   3.1s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 2), vect__stop_words=None, vect__tokenizer=None; total time=  10.8s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=None; total time=   3.6s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=None; total time=   3.7s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=None; total time=   3.2s\n",
      "[CV] END clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 2), vect__stop_words=None, vect__tokenizer=None; total time=  10.8s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 2), vect__stop_words=None, vect__tokenizer=None; total time=  14.5s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 2), vect__stop_words=None, vect__tokenizer=None; total time=  14.6s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 2), vect__stop_words=None, vect__tokenizer=None; total time=  14.9s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=None; total time=   2.9s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 2), vect__stop_words=None, vect__tokenizer=None; total time=  14.5s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=None; total time=   3.2s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=None; total time=   4.2s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=None; total time=   4.1s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__stop_words=None, vect__tokenizer=None; total time=   3.8s\n",
      "[CV] END clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 2), vect__stop_words=None, vect__tokenizer=None; total time=  12.5s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 2), vect__stop_words=None, vect__tokenizer=None; total time=  17.7s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 2), vect__stop_words=None, vect__tokenizer=None; total time=  17.5s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 2), vect__stop_words=None, vect__tokenizer=None; total time=  18.6s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 2), vect__stop_words=None, vect__tokenizer=None; total time=  21.1s\n",
      "[CV] END clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 2), vect__stop_words=None, vect__tokenizer=None; total time=  12.3s\n",
      "Best parameter set: {'clf__C': 10.0, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__tokenizer': None} \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "    'vect__stop_words': [None],\n",
    "    'vect__tokenizer': [None],  # Use default tokenizer\n",
    "    'clf__penalty': ['l1', 'l2'],\n",
    "    'clf__C': [1.0, 10.0],\n",
    "}\n",
    "\n",
    "# Initialize TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# Initialize Logistic Regression classifier\n",
    "clf = LogisticRegression(random_state=0, solver='liblinear')\n",
    "\n",
    "# Create a Pipeline\n",
    "lr_tfidf = Pipeline([\n",
    "    ('vect', tfidf),\n",
    "    ('clf', clf)\n",
    "])\n",
    "\n",
    "# Initialize GridSearchCV with multiprocessing\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=5, verbose=2,\n",
    "                           n_jobs=-1)  # Utilize all available CPU cores\n",
    "\n",
    "# Fit the model\n",
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameter set\n",
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy: 0.868\n",
      "Test Accuracy: 0.872\n"
     ]
    }
   ],
   "source": [
    "# Print the average 5-fold cross-validation accuracy score on the training dataset\n",
    "print('CV Accuracy: %.3f' % gs_lr_tfidf.best_score_)\n",
    "\n",
    "# Get the best estimator from the grid search\n",
    "clf = gs_lr_tfidf.best_estimator_\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_test = df.loc[5000:, 'review'].values\n",
    "y_test = df.loc[5000:, 'sentiment'].values\n",
    "\n",
    "# Print the classification accuracy on the test dataset\n",
    "print('Test Accuracy: %.3f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized\n",
    "\n",
    "def stream_docs(path):\n",
    "    with open(path, 'r', encoding='utf-8') as csv:\n",
    "        next(csv)  # skip header\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\"Calling this a romantic comedy is accurate but nowadays misleading. The genre has sadly deteriorated into cliches, too focused on making the main couple get together and with very little room for ambience and other stories, making it formulaic and overly predictable.<br /><br />The Shop Around the Corner does not suffer from these illnesses: it manages to create a recognisably middle/eastern-European atmosphere and has a strong cast besides the (also strong) nominal leads; I avoid using the words \\'supporting cast\\' as for example Mr. Matuschek (Frank Morgan) has a central role to the film and his story is equally if not more important than the romance.<br /><br />The 1998 film You\\'ve Got Mail borrowed the \\'anonymous pen-pal\\' idea from this film and has therefore been billed as a remake. This is not correct and in fact unfair to the new movie - it shares the genre and borrows a plot element, but that is all.\"',\n",
       " 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(stream_docs(path='/Users/jamilsharif/SentimentClassification/movie_data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "vect = HashingVectorizer(decode_error='ignore',\n",
    "                         n_features=2**21,\n",
    "                         preprocessor=None,\n",
    "                         tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SGDClassifier(loss='log_loss', random_state=1)\n",
    "\n",
    "doc_stream = stream_docs(path='/Users/jamilsharif/SentimentClassification/movie_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No valid output stream.\n"
     ]
    }
   ],
   "source": [
    "import pyprind\n",
    "pbar = pyprind.ProgBar(45)\n",
    "classes = np.array([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(45):\n",
    "    X_train, y_train = get_minibatch(doc_stream, size=1000)\n",
    "    if not X_train:\n",
    "        doc_stream = stream_docs(path='/Users/jamilsharif/SentimentClassification/movie_data.csv')  # Reset doc_stream\n",
    "        continue\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes=classes)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No test data available.\n"
     ]
    }
   ],
   "source": [
    "# Now, get the test data\n",
    "X_test, y_test = get_minibatch(doc_stream, size=5000)\n",
    "if X_test is not None:  # Check if test data exists\n",
    "    X_test = vect.transform(X_test)\n",
    "    print('Accuracy: %.3f' % clf.score(X_test, y_test))\n",
    "\n",
    "    # Update the model with the last 5,000 documents\n",
    "    clf = clf.partial_fit(X_test, y_test)\n",
    "else:\n",
    "    print(\"No test data available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/Users/jamilsharif/SentimentClassification/movie_data.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Assuming you have a sample input matrix X\n",
    "X = np.array([[1, 2, 3],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9]])\n",
    "\n",
    "# Define the number of components for NMF\n",
    "n_components = 2  # Adjust as needed\n",
    "\n",
    "# Initialize and fit NMF model\n",
    "nmf = NMF(n_components=n_components, random_state=123, max_iter=500)\n",
    "W = nmf.fit_transform(X)  # W represents document-topic matrix\n",
    "H = nmf.components_  # H represents topic-word matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assuming you have your original feature matrix X\n",
    "\n",
    "# Perform PCA for dimensionality reduction\n",
    "pca = PCA(n_components=3)  # Adjust the number of components based on the number of features\n",
    "X_reduced = pca.fit_transform(X)\n",
    "\n",
    "# Apply min-max scaling to ensure non-negative values\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = CountVectorizer(stop_words='english', max_df=0.1, max_features=5000)\n",
    "X = count.fit_transform(df['review'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 5  # Adjust as needed\n",
    "nmf = NMF(n_components=n_components, random_state=123, max_iter=500)\n",
    "X_topics_nmf = nmf.fit_transform(X)  # Assuming X is your input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMF Components Shape: (5, 5000)\n"
     ]
    }
   ],
   "source": [
    "print(\"NMF Components Shape:\", nmf.components_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "guy minutes action worst ll\n",
      "Topic 2:\n",
      "horror house gore blood effects\n",
      "Topic 3:\n",
      "series episode tv original episodes\n",
      "Topic 4:\n",
      "war american soldiers men german\n",
      "Topic 5:\n",
      "role family performance woman father\n"
     ]
    }
   ],
   "source": [
    "# Assuming nmf is your trained NMF model\n",
    "n_top_words = 5\n",
    "feature_names = count.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "    print(\"Topic %d:\" % (topic_idx + 1))\n",
    "    print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Horror movie #1:\n",
      "A DOUBLE LIFE has developed a mystique among film fans for two reasons: the plot idea of an actor getting so wrapped up into a role (here Othello) as to pick up the great flaw of that character and put it into his life; and that this is the film that won Ronald Colman the Academy Award (as well as t ...\n",
      "\n",
      "Horror movie #2:\n",
      "If you cannot enjoy a chick flick, stop right now. If, however, you enjoy films that illustrate complex characters and provide extraordinary acting, read on.<br /><br />Ann Grant Lord is dying. Her two daughters arrive to be at her bedside. Ann begins talking about people from her past of whom the d ...\n",
      "\n",
      "Horror movie #3:\n",
      "*!!- SPOILERS - !!*<br /><br />Before I begin this, let me say that I have had both the advantages of seeing this movie on the big screen and of having seen the \"Authorized Version\" of this movie, remade by Stephen King, himself, in 1997.<br /><br />Both advantages made me appreciate this version of ...\n"
     ]
    }
   ],
   "source": [
    "# Assuming X_topics_nmf is your topic distribution matrix obtained from NMF\n",
    "horror = X_topics_nmf[:, 4].argsort()[::-1]  # Accessing the 5th topic (index 4)\n",
    "for iter_idx, movie_idx in enumerate(horror[:3]):\n",
    "    print('\\nHorror movie #%d:' % (iter_idx + 1))\n",
    "    print(df['review'][movie_idx][:300], '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "dest = os.path.join('/Users/jamilsharif/SentimentClassification/movieclassifier/pkl_objects')\n",
    "if not os.path.exists(dest):\n",
    "    os.makedirs(dest)\n",
    "\n",
    "pickle.dump(stop,\n",
    "            open(os.path.join(dest, 'stopwords.pkl'), 'wb'),\n",
    "            protocol=4)\n",
    "\n",
    "pickle.dump(clf,\n",
    "            open(os.path.join(dest, 'classifier.pkl'), 'wb'),\n",
    "            protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "cur_dir = os.path.dirname('/Users/jamilsharif/SentimentClassification/movieclassifier/pkl_objects')\n",
    "stop = pickle.load(open(os.path.join(\n",
    "                   cur_dir, 'pkl_objects', 'stopwords.pkl'),\n",
    "                   'rb'))\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n",
    "                           text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) \\\n",
    "                    + ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized\n",
    "\n",
    "vect = HashingVectorizer(decode_error='ignore',\n",
    "                         n_features=2**21,\n",
    "                         preprocessor=None,\n",
    "                         tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import os\n",
    "from vectorizer import vect\n",
    "\n",
    "clf = pickle.load(open(os.path.join(\n",
    "                   'pkl_objects', 'classifier.pkl'),\n",
    "                   'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: positive\n",
      "Probability: 95.55%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "label = {0:'negative', 1:'positive'}\n",
    "example = [\"I love this movie. It's amazing.\"]\n",
    "X = vect.transform(example)\n",
    "\n",
    "print('Prediction: %s\\nProbability: %.2f%%' %\\\n",
    "      (label[clf.predict(X)[0]],\n",
    "       np.max(clf.predict_proba(X))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "\n",
    "# Create a connection to the SQLite database file\n",
    "conn = sqlite3.connect('reviews.sqlite')\n",
    "\n",
    "# Create a cursor object to execute SQL commands\n",
    "c = conn.cursor()\n",
    "\n",
    "# Drop the table if it already exists\n",
    "c.execute('DROP TABLE IF EXISTS review_db')\n",
    "\n",
    "# Create a new table named review_db\n",
    "c.execute('CREATE TABLE review_db'\n",
    "          ' (review TEXT, sentiment INTEGER, date TEXT)')\n",
    "\n",
    "# Insert example movie reviews into the database\n",
    "example1 = 'I love this movie'\n",
    "c.execute(\"INSERT INTO review_db\"\n",
    "          \" (review, sentiment, date) VALUES\"\n",
    "          \" (?, ?, DATETIME('now'))\", (example1, 1))\n",
    "\n",
    "example2 = 'I disliked this movie'\n",
    "c.execute(\"INSERT INTO review_db\"\n",
    "          \" (review, sentiment, date) VALUES\"\n",
    "          \" (?, ?, DATETIME('now'))\", (example2, 0))\n",
    "\n",
    "# Commit the changes to the database\n",
    "conn.commit()\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I love this movie', 1, '2024-05-01 01:49:06'), ('I disliked this movie', 0, '2024-05-01 01:49:06')]\n"
     ]
    }
   ],
   "source": [
    "# Reopen the connection to the SQLite database\n",
    "conn = sqlite3.connect('reviews.sqlite')\n",
    "c = conn.cursor()\n",
    "\n",
    "# Fetch all rows in the database table within a specific date range\n",
    "c.execute(\"SELECT * FROM review_db WHERE date\"\n",
    "          \" BETWEEN '2017-01-01 00:00:00' AND DATETIME('now')\")\n",
    "results = c.fetchall()\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n",
    "\n",
    "# Print the fetched results\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, render_template, request\n",
    "from wtforms import Form, TextAreaField, validators\n",
    "import pickle\n",
    "import sqlite3\n",
    "import os\n",
    "import numpy as np\n",
    "from vectorizer import vect  # Import HashingVectorizer from local dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_dir = os.path.dirname('/Users/jamilsharif/SentimentClassification/movieclassifier/pkl_objects')\n",
    "clf = pickle.load(open(os.path.join(cur_dir, 'pkl_objects', 'classifier.pkl'), 'rb'))\n",
    "db = os.path.join(cur_dir, 'reviews.sqlite')\n",
    "\n",
    "def classify(document):\n",
    "    label = {0: 'negative', 1: 'positive'}\n",
    "    X = vect.transform([document])\n",
    "    y = clf.predict(X)[0]\n",
    "    proba = np.max(clf.predict_proba(X))\n",
    "    return label[y], proba\n",
    "\n",
    "def train(document, y):\n",
    "    X = vect.transform([document])\n",
    "    clf.partial_fit(X, [y])\n",
    "\n",
    "def sqlite_entry(path, document, y):\n",
    "    conn = sqlite3.connect(path)\n",
    "    c = conn.cursor()\n",
    "    c.execute(\"INSERT INTO review_db (review, sentiment, date)\"\\\n",
    "              \" VALUES (?, ?, DATETIME('now'))\", (document, y))\n",
    "    conn.commit()\n",
    "    conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
